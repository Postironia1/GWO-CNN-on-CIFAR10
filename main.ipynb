import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import cifar10

calls = 0

# Загружаем данные CIFAR-10
(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

# Предобработка данных
train_images = train_images / 255.0
test_images = test_images / 255.0

# Оригинальный алгоритм
def GWO(obj_func, lb, ub, dim, search_agent_no, max_iter):
    global calls
    calls = 0

    alpha_pos = np.zeros(dim)
    beta_pos = np.zeros(dim)
    delta_pos = np.zeros(dim)
    alpha_score = float("inf")
    beta_score = float("inf")
    delta_score = float("inf")
    pos = np.zeros((search_agent_no, dim))
    for i in range(search_agent_no):
        pos[i, :] = np.random.uniform(lb, ub, dim)
    for l in range(max_iter):
        for i in range(search_agent_no):
            print(pos[i, :])
            fitness = obj_func(pos[i, :])
            if fitness < alpha_score:
                delta_score = beta_score
                delta_pos = beta_pos.copy()
                beta_score = alpha_score
                beta_pos = alpha_pos.copy()
                alpha_score = fitness
                alpha_pos = pos[i, :].copy()
            if fitness > alpha_score and fitness < beta_score:
                delta_score = beta_score
                delta_pos = beta_pos.copy()
                beta_score = fitness
                beta_pos = pos[i, :].copy()
        a = 2 * (1 - l / max_iter)
        for i in range(search_agent_no):
            r1 = np.random.rand(dim)
            r2 = np.random.rand(dim)
            A1 = 2 * a * r1 - a
            C1 = 2 * r2
            D_alpha = abs(C1 * alpha_pos - pos[i, :])
            X1 = alpha_pos - A1 * D_alpha
            r1 = np.random.rand(dim)
            r2 = np.random.rand(dim)
            A2 = 2 * a * r1 - a
            C2 = 2 * r2
            D_beta = abs(C2 * beta_pos - pos[i, :])
            X2 = beta_pos - A2 * D_beta
            r1 = np.random.rand(dim)
            r2 = np.random.rand(dim)
            A3 = 2 * a * r1 - a
            C3 = 2 * r2
            D_delta = abs(C3 * delta_pos - pos[i, :])
            X3 = delta_pos - A3 * D_delta
            pos[i, :] = (X1 + X2 + X3) / 3
            # check if position is within bounds
            pos[i, :] = np.clip(pos[i, :], lb, ub)
            # generate new delta_pos
            delta_pos = np.random.uniform(lb, ub, dim)
        # record the best fitness value for each iteration
        best_fitness = alpha_score
        # Check if the best fitness value is below a certain threshold
        print("Iteration: {}  x: {}".format(l + 1, alpha_pos))
    return alpha_pos, alpha_score, l + 1, calls


dim = 18  # Увеличим размерность для добавления коэффициента слоя Dropout и вероятности отсутствия слоя

search_agent_no = 30

max_iter = 20

# Генерация вероятности отсутствия слоя в диапазоне [0, 1]
lb = np.array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0])
ub = np.array([64, 1, 0.5, 1, 64, 1, 0.5, 1, 64, 1, 0.5, 1, 64, 1, 0.5, 1, 1, 0.5])

def neironka(x):
    global calls
    calls += 1
    R = 0
    # Создаем модель нейронной сети
    model = models.Sequential()

    # Первый сверточный слой всегда есть
    model.add(layers.Conv2D(int(x[0]), (3, 3), activation='relu', input_shape=(32, 32, 3)))
    model.add(layers.BatchNormalization())

    # Добавление сверточных слоев в соответствии с массивом x
    for i in range(1, dim - 2, 4):
        if x[i+1] > 0.5:  # Проверяем, есть ли слой
            model.add(layers.Conv2D(int(x[i]), (3, 3), activation='relu', padding='same'))
            model.add(layers.BatchNormalization())
            model.add(layers.Conv2D(int(x[i]), (3, 3), activation='relu', padding='same'))
            model.add(layers.BatchNormalization())
            model.add(layers.MaxPooling2D((2, 2)))
        if x[i+3] > 0.5 and x[i+1] > 0.5:  # Проверяем, есть ли слой
            # Добавление коэффициента слоя Dropout
            model.add(layers.Dropout(x[i+2]))

    model.add(layers.Flatten())

    # Добавление полносвязных слоев в соответствии с массивом x
    model.add(layers.Dense(int(x[i]), activation='relu'))
    if (x[dim - 2]) > 0.5:
        model.add(layers.Dropout(x[dim-1]))

    model.add(layers.Dense(10, activation='softmax'))

    # Компилируем модель
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    # Обучаем модель
    model.fit(train_images, train_labels, epochs=1)

    # Оцениваем модель на тестовых данных
    test_loss, test_acc = model.evaluate(test_images, test_labels)

    del model

    return test_loss

best_pos, best_score, iterations, calls = GWO(neironka, lb, ub, dim, search_agent_no, max_iter)

print("Best position:", best_pos)
print("Best score:", best_score)
